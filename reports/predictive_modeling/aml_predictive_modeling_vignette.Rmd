---
title: "AML Predictive Modeling Vignette" 
author: tkeyes
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi = 300)
```

```{r message=FALSE, warning=FALSE}
# Libraries
libraries <- 
  c(
    "tidyverse", 
    "tidymodels", 
    "rlang", 
    "ggthemes", 
    "doParallel", 
    "vip", 
    "tidytof"
  )

source(here::here("scripts", "setup", "aml_utils.R")) 
call_libraries(libraries)

# Parameters
set_global_variables(locale = "galaxia")

CLUSTER_OUTPUT <- here::here("data", "cancer_data_clustered.rds")

# set up aml marker and patient information
marker_setup()
patient_setup()

# Misc globals for running different parts of this vignette
mah_dev_path <- here::here("data", "mahalanobis_dev_features_10.rds")
cos_dev_path <- here::here("data", "cosine_dev_features_10.rds")
mah_citrus_path <- here::here("data", "mahalanobis_citrus_features.rds")
cos_citrus_path <- here::here("data", "cosine_citrus_features.rds")
md_path <- here::here("data", "md.rds")

ddpr_feature_path <- 
  here::here("data", "patient-level_feature_matrices", "ddpr_feature_matrices.rds")
mean_feature_path <- 
  here::here("data", "patient-level_feature_matrices", "mean_feature_matrices_sampled.rds")
emd_feature_path <- 
  here::here("data", "patient-level_feature_matrices", "emd_feature_matrices.rds")
js_feature_path <- 
  here::here("data", "patient-level_feature_matrices", "js_feature_matrices.rds")


DDPR_PROP <- 0.8

# set a randomness seed to make everything reproducible 
seed <- 12
set.seed(seed)
```

# Reading in data

First, I read in the data that I've extracted from the AML dataset. (I also load some other datasets here that I've played with, but I don't use them in the current analysis...yet.)

The data have already been pre-processed according to the procedure described [here](https://github.com/keyes-timothy/aml-cells/blob/master/reports/aml_intro_vignette.md), and patient-level features have been extracted from the single-cell data according to the procedure described [here](https://github.com/keyes-timothy/aml-cells/blob/master/reports/aml_feature_extraction_vignette.md). 

## First gen feature extraction

```{r}
# read in metadata
metadata <-
  md_path %>%
  read_rds() %>%
  transmute(
    patient, 
    age = age_at_diagnosis_in_days, 
    wbc = wbc_at_diagnosis, 
    outcome = first_event %>% str_replace("Censored", "Non-relapse"), 
    fab_category, 
    cns_disease, 
    primary_cytogenetic_code, 
    cytogenetic_complexity
  )

# reading in mahalanobis distance-classified AML data
mah_dev_data <- 
  mah_dev_path %>% 
  read_rds() %>% 
  filter(condition == "dx") %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)

# reading in cosine distance-classified AML data
cos_dev_data <- 
  cos_dev_path %>% 
  read_rds() %>% 
  filter(condition == "dx") %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)

# reading in mahalanobis distance-classified citrus data
mah_citrus_data <- 
  mah_citrus_path %>% 
  read_rds() %>% 
  filter(condition == "dx") %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)

# reading in cosine distance-classified citrus data
cos_citrus_data <- 
  cos_citrus_path %>% 
  read_rds() %>% 
  filter(condition == "dx") %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)
```


## Second-gen feature extraction

```{r}
all_feature_matrices <- 
  tibble(
    method = c(ddpr_feature_path, mean_feature_path, emd_feature_path, js_feature_path), 
    matrix = map(.x = method, .f = read_rds)
  ) %>% 
  mutate(
    method = 
      str_remove(
        string = method, 
        pattern = here::here("data", "patient-level_feature_matrices/")
      ) %>% 
      str_extract("^[:alpha:]+")
  )

all_feature_matrices
```


# Very quick EDA

Before any predictive modeling, it's good practice to do a little bit of exploratory data analysis (EDA) just to make sure you know what you're looking at and what you should expect from your model. I give what amounts to essentially the bare minimum of EDA here just to illustrate a few points that are on my mind about the AML dataset...

## What samples do we have? 

```{r}
mah_dev_data %>% 
  count(condition, outcome) %>% 
  knitr::kable() 
```

Okay, so we have 29 samples in total, and 12 of them relapse (17 of them don't). So, this cohort is relatively small and should mostly be used for **discovery**, or hypothesis-generation. This will inform some of our modeling choices later, since we don't really have enough samples to do a true train/test validation in the most rigorous sense. 


## Mahalanobis developmental classifier features 

Here are some PCA plots showing how well our samples can be separated in two dimensions based on their extracted DDPR features. 

```{r}
# needs to show the percentage variance for each PC in the plot
pca_recipe <- 
  recipe(
    ~., 
    data = 
      mah_dev_data %>% 
      replace(list = is.na(.), values = 0)
  ) %>% 
  step_zv(all_numeric()) %>% 
  step_normalize(all_numeric()) %>% 
  step_pca(all_numeric(), num_comp = 5) 

pca_prep <- prep(pca_recipe)

pca_result <- juice(pca_prep)

pca_result %>% 
  ggplot(aes(x = PC1, y = PC2, color = outcome)) + 
  geom_point(size = 3, alpha = 0.7) + 
  labs(title = "Developmental Classifier Features using\nMahalanobis Distance")
```

And here is some code that plots features in a way that we can try to interpret later (we will return to this after the modeling steps). 

```{r eval=FALSE, warning=FALSE}
mah_dev_data %>% 
  select(where(is_numeric), patient, condition, outcome) %>% 
  pivot_longer(
    cols = c(-patient, -condition, -outcome), 
    names_to = "feature", 
    values_to = "value"
  ) %>% 
  group_by(feature) %>% 
  nest() %>%
  mutate(
    plots = 
      map2(
        .x = data, 
        .y = feature,
        .f = 
          ~ ggplot(
            aes(x = fct_reorder(patient, value), y = value, color = outcome), 
            data = .x
          ) + 
          geom_point() + 
          coord_flip() + 
          labs(title = feature, x = "patient", y = "expression level")
      )
  ) %>% 
  pull(plots) %>% 
  `[`(seq(1, length(colnames(mah_dev_data)), 100))

```

From these more or less randomly sampled plots, I hope to illustrate a trend for many of our features - that most of them are left-skewed (aka most patients have relatively low values within the same(ish) range, but several patients will be quite a bit higher than the others on each feature). 

Here is a bit more evidence of this: 

```{r, message = FALSE, warning = FALSE}
density_data <- 
  mah_dev_data %>% 
  dplyr::mutate(across(.cols = where(is.numeric), .fns = scale)) %>%
  select(where(is.numeric)) %>% 
  pivot_longer(
    cols = everything(), 
    names_to = "channel", 
    values_to = "expression"
  ) 

density_data %>% 
  ggplot(aes(x = expression, group = channel)) + 
  geom_density(size = 0.1) + 
  geom_density(data = density_data, mapping = aes(x = expression, group = NULL), color = "red")
```

## Cosine developmental classifier features

We can make the same PCA plot as above for the classifier data that was classified using the cosine distance between cells and each manually-gated healthy population rather than the Mahalanobis distance. 

```{r}
# needs to show the percentage variance for each PC in the plot
pca_recipe <- 
  recipe(
    ~., 
    data = 
      cos_dev_data %>% 
      replace(list = is.na(.), values = 0)
  ) %>% 
  step_zv(all_numeric()) %>% 
  step_normalize(all_numeric()) %>% 
  step_pca(all_numeric(), num_comp = 5) 

pca_prep <- prep(pca_recipe)

pca_result <- juice(pca_prep)

pca_result %>% 
  ggplot(aes(x = PC1, y = PC2, color = outcome)) + 
  geom_point(size = 3, alpha = 0.7)  + 
  labs(title = "Developmental Classifier Features using Cosine Distance")
```


```{r}
density_data <- 
  cos_dev_data %>% 
  mutate(across(.cols = where(is.numeric), scale)) %>%
  dplyr::select(where(is.numeric)) %>% 
  pivot_longer(
    cols = everything(), 
    names_to = "channel", 
    values_to = "expression"
  ) 

density_data %>% 
  ggplot(aes(x = expression, group = channel)) + 
  geom_density(size = 0.1) + 
  geom_density(data = density_data, mapping = aes(x = expression, group = NULL), color = "red")
```

My point in showing these plots for the cosine data (even though we're really not going to look at it again in this presentation) is to illustrate that it doesn't seem to just be the Mahalanobis distance that introduces these properties to our feature matrices - they seem to be a property of the data (as represented by these feature extractions). 

With this small amount of EDA behind us, we can move on to the modeling steps. 

# Split data 

Because this dataset has been earmarked for discovery, we are not going to divide the data into a traditional "training" and "test" set for this specific task. Instead, we are going to break up the data into 5 cross-validation folds, wherein each fold will treat 80% of the data as an **analysis set** that will be used to fit the model and 20% of the data as an **assessment set** that will be used to evaluate the model. (5 is an arbitrary number of cv folds, by the way. You can choose more or less if you want.)

If we repeat this cross-validation strategy multiple times (**repeated cross-validation**), we can try to get around our small sample size by simulating access to multiple datasets. This overall procedure is sometimes referred to as "resampling" in the statistics world. The package {rsample} makes this process really painless in R.

```{r}
#bootstrapping approach
mah_dev_split <- 
  initial_split(mah_dev_data, strata = outcome)
  
mah_dev_train <- training(mah_dev_split)

mah_dev_test <- testing(mah_dev_split)

mah_dev_boots <- 
  mah_dev_data %>% 
  bootstraps(times = 1000, strata = outcome)

mah_dev_boots


# cross validation approach 
mah_dev_cv <- 
  mah_dev_data %>% 
  vfold_cv(v = 5, repeats = 100, strata = outcome) 

cos_dev_cv <- 
  cos_dev_data %>% 
  vfold_cv(v = 5, repeats = 100, strata = outcome)

mah_dev_cv
```

```{r}
resampling_df <- 
  mah_dev_boots %>% 
  tidy() %>% 
  mutate(
    resample = str_sub(Resample, start = -3L) %>% as.integer(), 
    Data = fct_recode(Data, "Training" = "Analysis", "Test" = "Assessment")
  ) %>% 
  filter(resample <= 20) 

resampling_figure <- 
  resampling_df %>% 
  ggplot(aes(x = resample, y = Row, fill = Data)) + 
  geom_tile(color = "black", size = 0.4) + 
  geom_linerange(
    aes(
      ymin = min(Row) - 0.55, 
      ymax = max(Row) + 0.55
    ), 
    x = 0 + 0.5, 
    size = 3, 
    color = "white"
  ) + 
  geom_linerange(
    aes(
      ymin = min(Row) - 0.55, 
      ymax = max(Row) + 0.55,
      x = resample + 0.5
    ), 
    size = 3, 
    color = "white"
  ) + 
  labs(
    x = "Bootstrap iteration", 
    y = "Patient", 
    fill = NULL
  ) + 
  theme_minimal() + 
  theme(
    axis.title = element_text(size = 20), 
    axis.text = element_text(size = 12)
  )

ggsave(
  filename = "resampling.pdf", 
  plot = resampling_figure, 
  path = file.path("~", "Desktop"), 
  width = 10, 
  height = 6
)

resampling_figure

  
```


This data structure will come in handy later. 

# Set up the modeling objects

I've built my modeling framework in the [{tidymodels}](https://www.tidymodels.org/) ecosystem, which is a powerful suite of R packages for preprocessing, modeling, post-processing, and interpreting machine learning models in R. There are a lot of great design principles here, which is why I'm using it for the modelings tasks here.

There's a [new free book](https://www.tmwr.org/) out about tidymodels, which you should check out if you're interested. Or I can just do all of the modeling for your project ;)

First, I use the tidymodels package `{recipes}` to build a reproducible preprocessing framework for the data. Then, I use `{dials}` to create a grid (using maximum entropy) over which to search for the optimal elastic net parameters. Then, I use the modeling package {parsnip} to make the model specification (a logistic regression model with elastic net regularization). Finally, I incorporate each of these steps into a single workflow using the package {workflows}. 

```{r}
# preprocessing
ddpr_dev_recipe <- 
  mah_dev_data %>% 
  aml_create_recipe(type = "full")

# create grid for parameter tuning
en_param_grid <-
  parameters(penalty()) %>%
  grid_regular(levels = 50)
  
  #, mixture()) %>% 
  #grid_max_entropy(size = 100)
  # an alternative would be a regular grid, but this is slower
  # grid_regular(levels = c(5, 5))

# model specification
ddpr_en_model_spec <- 
  logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>% 
  set_mode("classification")

# overall workflow
ddpr_en_workflow <- 
  workflow() %>% 
  add_recipe(ddpr_dev_recipe) %>% 
  add_model(ddpr_en_model_spec)

# null model
null_recipe <-
  mah_dev_data %>% 
  transmute(
    outcome = as.factor(outcome), 
    age, 
    wbc
  ) %>%
  recipe(outcome ~ .) %>% 
  step_normalize(-outcome)

ddpr_null_model_spec <- 
  logistic_reg(penalty = 0, mixture = 0) %>%
  set_engine("glmnet") %>% 
  set_mode("classification")
  
ddpr_null_workflow <- 
  workflow() %>% 
  add_recipe(null_recipe) %>% 
  add_model(ddpr_null_model_spec)
  
```

```{r}
ddpr_dev_recipe

en_param_grid
 
ddpr_en_model_spec

ddpr_en_workflow
```


# Tune the model

With these initial pieces, I can use the {tune} package to fit thousands of models over both my cross validation folds and my candidate values for both of my elastic net parameters to find the optimal hyperparameters. By running this step in parallel on multiple cores on my laptop, I can save some time. 

```{r}
# Note that tidymodels runs on a {foreach} backend, 
# which means that we can register a parallel worker to 
# speed up our processing speed quite a bit 

my_cluster <- makeCluster(14)
registerDoParallel(my_cluster)

# cv approach 
ddpr_en_tune <- 
  tune_grid(
    ddpr_en_workflow, 
    resamples = mah_dev_cv, 
    grid = en_param_grid
  )

stopCluster(my_cluster)

# bootstrapping approach

my_cluster <- makeCluster(14)
registerDoParallel(my_cluster)

ddpr_en_tune_boot <- 
  tune_grid(
    ddpr_en_workflow, 
    resamples = mah_dev_boots, 
    grid = en_param_grid
  )

stopCluster(my_cluster)

```

```{r}

# cross validation approach
ddpr_en_tune %>% 
  # will give the average for each model across all repetitions and all folds
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean, color = .metric)) + 
  geom_errorbar(
    aes(ymin = mean - std_err, ymax = mean + std_err), 
    alpha = 0.5
  ) + 
  geom_line(size = 1.5) + 
  facet_grid(rows = vars(.metric), scales = "free") + 
  scale_x_log10()


### bootstrapping approach

tuning_curve <- 
  ddpr_en_tune_boot %>% 
  # will give the average for each model across all repetitions and all folds
  collect_metrics() %>% 
  mutate(.metric = fct_recode(.metric, "AuROC" = "roc_auc")) %>% 
  ggplot(aes(x = penalty, y = mean, color = .metric)) + 
  geom_errorbar(
    aes(ymin = mean - std_err, ymax = mean + std_err), 
    alpha = 0.5
  ) + 
  geom_line(size = 1.5) + 
  facet_grid(rows = vars(.metric), scales = "free") + 
  scale_x_log10() + 
  theme(legend.position = "None") + 
  labs(
    y = NULL, 
    color = NULL, 
    caption = "Performance metrics are computed across all bootstrap resamples"
  )

tuning_curve

ggsave(
  filename = "tuning_curve.pdf", 
  plot = tuning_curve, 
  path = file.path("~", "Desktop"), 
  width = 6, 
  height = 5
)

ddpr_en_tune_boot %>% 
  collect_metrics()
  
```


```{r}
### cross validation approach

highest_accuracy <- 
  ddpr_en_tune %>% 
  select_best(metric = "accuracy")

highest_auc <- 
  ddpr_en_tune %>% 
  select_best(metric = "roc_auc")

highest_accuracy

highest_auc

```

```{r}
### bootstrapping approach

highest_accuracy_boot <- 
  ddpr_en_tune_boot %>% 
  select_best(metric = "accuracy")

highest_auc_boot <- 
  ddpr_en_tune_boot %>% 
  select_best(metric = "roc_auc")

highest_accuracy_boot

highest_auc_boot

```

```{r}
### cross validation approach
final_lasso <- 
  ddpr_en_workflow %>% 
  finalize_workflow(parameters = highest_auc)

final_lasso

### bootstrapping approach

final_lasso_boot <- 
  ddpr_en_workflow %>% 
  finalize_workflow(parameters = highest_auc_boot)

final_lasso_boot

```

# Fitting the whole model(s) 

```{r}
### cross validation
feature_importance <- 
  final_lasso %>% 
  fit(data = mah_dev_data) %>% 
  pull_workflow_fit() %>% 
  vi(lambda = highest_auc$penalty) %>% 
  mutate(Importance = abs(Importance), Variable = fct_reorder(Variable, Importance)) %>% 
  filter(Importance > 0)

feature_importance %>% 
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) + 
  geom_col() + 
  scale_x_continuous(expand = c(0, 0))



### bootstrapping approach

feature_importance_boot <- 
  final_lasso_boot %>% 
  fit(data = mah_dev_train) %>% 
  pull_workflow_fit() %>% 
  vi(lambda = highest_auc$penalty) %>% 
  mutate(Importance = abs(Importance), Variable = fct_reorder(Variable, Importance)) %>% 
  filter(Importance > 0)

feature_importance_plot <- 
  feature_importance_boot %>% 
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) + 
  geom_col() + 
  scale_x_continuous(expand = c(0, 0)) + 
  labs(
    y = NULL
  )

ggsave(
  filename = "feature_importance_plot.pdf", 
  plot = feature_importance_plot, 
  path = file.path("~", "Desktop"), 
  width = 4, 
  height = 4
)

feature_importance_plot


```



The most important features...

```{r}
# cross validation
coefficient_names <- 
  feature_importance %>% 
  pull(Variable) %>% 
  as.character()

coefficient_names

# bootstrap approach

coefficient_names_boot <- 
  feature_importance_boot %>% 
  pull(Variable) %>% 
  as.character()

coefficient_names_boot


```


```{r}
coefficient_names_boot %>% 
  write_rds(
    x = ., 
    file = 
      here::here(
        "data", 
        "modeling_logistic", 
        str_c("mah_dev_feature_names_boot_", seed, "_", ".rds")
      )
  )
```


And a visualization of the most predictive features: 

```{r}
mah_dev_data %>% 
  select(where(is.numeric), patient, condition, outcome) %>%
  pivot_longer(
    cols = c(-patient, -condition, -outcome), 
    names_to = "feature", 
    values_to = "value"
  ) %>% 
  group_by(feature) %>% 
  nest() %>%
  mutate(
    plots = 
      map2(
        .x = data, 
        .y = feature,
        .f = 
          ~ ggplot(
            aes(x = fct_reorder(patient, value), y = value, color = outcome), 
            data = .x
          ) + 
          geom_point() + 
          coord_flip() + 
          labs(title = feature, x = "patient", y = "expression level")
      )
  ) %>% 
  filter(feature %in% coefficient_names_boot) %>% 
  pull(plots) %>% 
  walk(print)
```

From there, we can use our models that were fit on the entire dataset and see where they performed best.  

```{r}

perf_metrics <- metric_set(accuracy, sens, spec, f_meas)


# null model 

roc_predictions_null <-
  mah_dev_boots %>% 
  mutate(
    predictions = 
      map(
        .x = splits, 
        .f = ~ 
          fit(ddpr_null_workflow, data = select(training(.x), outcome, wbc, age)) %>% 
          predict(new_data = select(testing(.x), age, wbc, outcome), type = "prob") %>% 
          transmute(
            estimate = .pred_Relapse, 
            truth = factor(testing(.x)$outcome, levels = c("Relapse", "Non-relapse"))
          )
      ), 
    class = map(
      .x = splits,
      .f = ~
        fit(ddpr_null_workflow, data = select(training(.x), outcome, wbc, age)) %>%
        predict(new_data = select(testing(.x), age, wbc, outcome), type = "class") %>%
        transmute(
          class_prediction = .pred_class,
        )
    )
  ) %>% 
  unnest(cols = c(predictions, class))


# actual model
roc_predictions <- 
  mah_dev_boots %>% 
  mutate(
    predictions = 
      map(
        .x = splits, 
        .f = ~ 
          fit(final_lasso_boot, data = training(.x)) %>% 
          predict(new_data = testing(.x), type = "prob") %>% 
          transmute(
            estimate = .pred_Relapse, 
            truth = factor(testing(.x)$outcome, levels = c("Relapse", "Non-relapse"))
          )
      ), 
    class = map(
      .x = splits,
      .f = ~
        fit(final_lasso_boot, data = training(.x)) %>%
        predict(new_data = testing(.x), type = "class") %>%
        transmute(
          class_prediction = .pred_class,
        )
    )
  ) %>% 
  unnest(cols = c(predictions, class))

```

```{r}
# plot ROC curves

null_roc <- 
  roc_predictions_null %>% 
  roc_curve(truth, estimate) %>% 
  ggplot(aes(x = 1 - specificity, sensitivity)) + 
  geom_path() + 
  geom_abline(lty = 3) + 
  coord_equal() + 
  theme_bw() + 
  labs(
    title = "Baseline model ROC curve", 
    subtitle = "Model only includes patient age and WBC count"
  )

boot_roc <- 
  roc_predictions %>% 
  roc_curve(truth, estimate) %>% 
  ggplot(aes(x = 1 - specificity, sensitivity)) + 
  geom_path() + 
  geom_abline(lty = 3) + 
  coord_equal() + 
  theme_bw() + 
  labs(
    title = "Lasso model ROC curve", 
    subtitle = "Model includes Lasso-identified features"
  )

null_roc

boot_roc

# save ROC curves

ggsave(
  filename = "null_roc.pdf", 
  plot = null_roc, 
  path = file.path("~", "Desktop"), 
  width = 5, 
  height = 5
)

ggsave(
  filename = "boot_roc.pdf", 
  plot = boot_roc, 
  path = file.path("~", "Desktop"), 
  width = 5, 
  height = 5
)

bootstrap_performance_metrics <- 
  roc_predictions %>% 
  perf_metrics(truth = fct_rev(truth), estimate = class_prediction) %>% 
  mutate(model = "lasso")

null_performance_metrics <- 
  roc_predictions_null %>% 
  perf_metrics(truth = fct_rev(truth), estimate = class_prediction) %>% 
  mutate(model = "null") 

performance_metrics <- 
  null_performance_metrics %>% 
  bind_rows(bootstrap_performance_metrics) %>% 
  select(-.estimator) #%>% 
  # pivot_wider(
  #   names_from = .metric, 
  #   values_from = .estimate
  # )


```

```{r}
performance_metrics_plot <- 
  performance_metrics %>% 
  mutate(
    .metric = 
      fct_recode(.metric, "sensitivity" = "sens", "specificity" = "spec", "F-measure" = "f_meas") %>% 
      fct_relevel("accuracy", "sensitivity", "specificity", "F-measure")
  ) %>% 
  ggplot(aes(x = model, y = .estimate, fill = model)) + 
  geom_col() + 
  facet_grid(cols = vars(.metric), scales = "free") + 
  labs(
    x = NULL, 
    y = NULL, 
    caption = "Calculated on all OOB observations across all bootstrap samples"
  )

ggsave(
  filename = "performance_metrics.pdf", 
  plot = performance_metrics_plot, 
  path = file.path("~", "Desktop"), 
  width = 5, 
  height = 4
)


performance_metrics_plot



```

In general, we can see that, on average, each of the resampled models (when rerun on all the data) give us good sensitivity and *decent* specificity. Recall that 

* Sensitivity refers to the number of **True Positives** that are captured by the model (i.e. proportion of True Positives that the model labels as positive) 
* Specificity refers to the number of **True Negatives** that are captured by the model (i.e. proportion of True Negatives that the model labels as negative). 

Thus, our model has a bias for labeling kids as having a risk of relapse even if they don't. 

Using our favorite features, we can now rerun our PCA and see if the patients segregate better using just our favorite features: 

```{r}
pca_recipe <- 
  recipe(
    ~., 
    data = 
      mah_dev_data %>% 
      select(all_of(c("outcome", coefficient_names))),
  ) %>% 
  step_zv(all_numeric()) %>% 
  step_normalize(all_numeric()) %>% 
  step_pca(all_numeric(), num_comp = 5)

pca_prep <- prep(pca_recipe)

pca_result <- juice(pca_prep)

pca_result %>% 
  mutate(outcome = fct_rev(outcome)) %>% 
  ggplot(aes(x = PC1, y = PC2, color = outcome)) + 
  geom_point(size = 3, alpha = 0.7) + 
  labs(title = "Developmental Classifier Features using \nMahalanobis Distance")
```


Finally, we store the vector containing the name of the most-predictive features in its own file so that we can reference it later. 

```{r}
coefficient_names_boot %>% 
  write_rds(
    x = ., 
    file = 
      here::here(
        "data", 
        "modeling_logistic", 
        str_c("mah_dev_feature_names_boot_", seed, "_", ".rds")
      )
  )
```


# Survival modeling with the same features

```{r}
metadata <- 
  md_path %>% 
  read_rds() %>% 
  transmute(
    patient, 
    event_free_survival_time = event_free_survival_time_in_days,
    first_event,
    aml0531_risk = 
      case_when(
        inv_16 == "Yes" | t_8_21 == "Yes" ~ "Low", 
        any(
          monosomy_5 == "Yes", 
          monosomy_7 == "Yes", 
          del5q == "Yes", 
          !is.na(flt3_itd_allelic_ratio)
        )                                 ~ "High", 
        TRUE                              ~ "Intermediate"
      )
  )

aml_data <- 
  mah_dev_data %>% 
  left_join(metadata) %>% 
  select(all_of(coefficient_names_boot), event_free_survival_time, first_event, age) %>% 
  mutate(
    across(all_of(coefficient_names_boot), scale), 
    first_event = first_event != "Censored"
  ) #%>% 
  #rename(DC_HLA_DR = `DC_HLA-DR`)

aml_data
```

```{r}
library(survival)
library(survminer)

surv_model <- 
  surv_reg() %>% 
  set_engine("survival")

# full model

surv_fit <- 
  surv_model %>% 
  fit(
    formula = 
      reformulate(
        setdiff(colnames(aml_data), c("event_free_survival_time", "first_event")), 
        response = "Surv(event_free_survival_time, first_event)"
      ),
    data = aml_data
  )

surv_predictions <- 
  surv_fit %>% 
  predict(new_data = aml_data) %>% 
  mutate(
    patient = mah_dev_data$patient, 
    DDPR_high = .pred < median(.pred)
  )

surv_fit %>% 
  tidy() %>% 
  filter(p.value < 0.05)

# null model 


null_surv_fit <- 
  surv_model %>% 
  fit(
    formula = Surv(event_free_survival_time, first_event) ~ age + wbc, 
    data = aml_data
  )

null_surv_predictions <- 
  null_surv_fit %>% 
  predict(new_data = aml_data) %>% 
  mutate(
    patient = mah_dev_data$patient, 
    null_high = .pred < median(.pred)
  )

surv_fit %>% 
  tidy() %>% 
  filter(p.value < 0.05)

```


```{r}

# full model

surv_fit_plot <- 
  survfit(
    formula = Surv(event_free_survival_time, first_event) ~ DDPR_high, 
    data = 
      aml_data %>% 
      mutate(patient = mah_dev_data$patient) %>% 
      left_join(surv_predictions) %>% 
      left_join(metadata %>% select(-first_event, -event_free_survival_time))
  )

surv_fit_plot %>% 
  ggsurvplot(
    size = 1, 
    conf.int = TRUE, 
    pval = TRUE 
  )

surv_formula <- 
  reformulate(
        setdiff(colnames(aml_data), c("event_free_survival_time", "first_event")), 
        response = "Surv(event_free_survival_time, first_event)"
      )

surv_fit_plot <- 
  survfit(
    formula = surv_formula, 
    data = aml_data
  )
 
surv_fit_plot_figure <- 
  surv_fit_plot %>% 
  ggsurvplot(
    size = 1, 
    conf.int = TRUE, 
    pval = TRUE, 
    ggtheme = theme_bw()
  )

surv_fit_plot_figure

ggsave(filename = "surv_fit_plot.pdf", device = "pdf", path = file.path("~", "Desktop"), height = 5, width = 8)

# null model 

surv_fit_plot_null <- 
  survfit(
    formula = Surv(event_free_survival_time, first_event) ~ null_high, 
    data = 
      aml_data %>% 
      mutate(patient = mah_dev_data$patient) %>% 
      left_join(null_surv_predictions) %>% 
      left_join(metadata %>% select(-first_event, -event_free_survival_time))
  )

surv_fit_plot_figure_null <- 
  surv_fit_plot_null %>% 
  ggsurvplot(
    size = 1, 
    conf.int = TRUE, 
    pval = TRUE, 
    ggtheme = theme_bw()
  )



surv_fit_plot_figure_null

ggsave(filename = "surv_fit_plot_null.pdf", device = "pdf", path = file.path("~", "Desktop"), height = 5, width = 8)
  

```


# plotting features

```{r}
full_data <- 
  mah_dev_path %>% 
  read_rds() %>% 
  filter(condition %in% c("dx", "rx")) %>% 
  select(any_of(c(coefficient_names_boot, "patient", "condition"))) %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)

diagnosis_feature_plot <- 
  full_data %>% 
  mutate(across(any_of(coefficient_names_boot), scale, center = FALSE)) %>% 
  filter(condition == "dx") %>% 
  pivot_longer(
    cols = any_of(coefficient_names_boot), 
    names_to = "feature", 
    values_to = "value"
  ) %>% 
  group_by(feature, first_event) %>% 
  summarize(
    sd = sd(value),
    value = mean(value), 
    sem = sd / sqrt(n())
  ) %>% 
  #mutate(
    #feature = str_replace(feature, pattern = "Thrombocyte", replacement = "Thromb"), 
    #feature = fct_reorder(feature, value)
  #) %>% 
  ggplot(aes(x = first_event, y = value, fill = first_event)) + 
  geom_col(position = "dodge") + 
  geom_errorbar(aes(ymin = value - sem, ymax = value + sem), width = 0.4, alpha = 0.7) + 
  facet_wrap(facets = vars(feature), nrow = 2) + 
  theme_bw() + 
  theme(
    axis.ticks.x = element_blank(), 
    axis.text.x = element_blank(), 
    legend.position = "bottom"
  ) + 
  labs(
    y = "Mean Feature Value (scaled to SD=1)", 
    x = NULL, 
    fill = NULL, 
    caption = "Error bars indicate SEMs"
  )

diagnosis_feature_plot

ggsave(
  filename = "diagnosis_feature_plot.pdf", 
  plot = diagnosis_feature_plot, 
  path = file.path("~", "Desktop"), 
  width = 12, 
  height = 8
)



# do features persist during relapse 

relapse_feature_plot <- 
  full_data %>% 
  mutate(across(any_of(coefficient_names_boot), scale, center = FALSE)) %>% 
  group_by(patient) %>% 
  filter(n() > 1) %>% 
  pivot_longer(
    cols = any_of(coefficient_names_boot), 
    names_to = "feature", 
    values_to = "value"
  ) %>% 
  group_by(feature, condition) %>% 
  summarize(
    sd = sd(value),
    value = mean(value), 
    sem = sd / sqrt(n())
  ) %>% 
  #mutate(feature = fct_reorder(feature, value)) %>% 
  ggplot(aes(x = condition, y = value, fill = condition)) + 
  geom_col(position = "dodge") + 
  geom_errorbar(aes(ymin = value - sem, ymax = value + sem), width = 0.4, alpha = 0.7) + 
  facet_wrap(facets = vars(feature), nrow = 2) + 
  theme_bw() + 
  theme(
    axis.ticks.x = element_blank(), 
    axis.text.x = element_blank(), 
    legend.position = "bottom"
  ) + 
  labs(
    y = "Mean Feature Value (scaled to SD=1)", 
    x = NULL, 
    fill = NULL, 
    caption = "Error bars indicate SEMs"
  )

relapse_feature_plot

ggsave(
  filename = "relapse_feature_plot.pdf", 
  plot = relapse_feature_plot, 
  path = file.path("~", "Desktop"), 
  width = 12, 
  height = 8
)




```

```{r}
relapse_feature_plot_2 <- 
  full_data %>% 
  mutate(across(any_of(coefficient_names_boot), scale, center = FALSE)) %>% 
  group_by(patient) %>% 
  filter(n() > 1) %>% 
  pivot_longer(
    cols = any_of(coefficient_names_boot), 
    names_to = "feature", 
    values_to = "value"
  ) %>% 
  ggplot(aes(x = condition, y = value, fill = patient)) + 
  geom_line(aes(group = patient), alpha = 0.7) + 
  geom_point(shape = 21, size = 3) + 
  facet_wrap(facets = vars(feature), nrow = 2) + 
  theme_bw() + 
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) + 
  labs(
    y = "Feature Value (scaled to SD=1)", 
    x = NULL, 
    fill = NULL 
  )

relapse_feature_plot_2

ggsave(
  filename = "relapse_feature_plot_2.pdf", 
  plot = relapse_feature_plot_2, 
  path = file.path("~", "Desktop"), 
  width = 12, 
  height = 8
)

```

# Survival modeling from scratch

Regularized cox modeling is not yet supported by the `tidymodels` framework, which means that I have to use `glmnet` from scratch to feature select while fitting the survival model itself. To do so, I used [this tutorial]("https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf") to help me out. 

## What is Cox modeling?

Insert section about the mathematical theory behind Cox regression.

```{r}

```


#  Future Directions

The frustrating thing about being a bioinformatician-in-training (at least according to me) is that my "taste" in data science has matured much faster than my technical ability. So, what I've presented here represents only a very, very small degree of what I'm actually interested in looking at even for this tiny corner of my project! 

Here are the other things on my mind: 

* Looking at other feature matrices
  * Compare feature matrices extracted using the DDPR methodology to those extracted using CITRUS/Scaffold methodolody
  * Compare the performance of feature matrices extracted using the DDPR classifier to classifiers built with flowSOM and PhenoGraph clustering
  * Compare the performance of feature matrices extracted using the DDPR classifier to matrices extracted using clustering on its own (without referencing the healthy spaces) 
  * Build my own custom feature matrices using mutual information or the EMD to quantify signaling variables in a more subtle way. 
  * Regardless of which of these feature matrices I use, it makes sense for me to think about performing a box-cox transformation 
  
* Deep learning 
  * I want to do this so badly 
  * We are limited in power in terms of patients, but not in terms of cells. The single-cell information could be very powerful here. 
  * Finding developmental feature embeddings on a single-cell level will be much more possible (and cooler) with deep learning approaches - because we will avoid losing much of the resolution that we lose here
  * I want to do this so so so so badly 
  
* Review of theory
  * I need to do a bit more reading to adapt these models from logistic regression to surival regression (Cox modeling), which is more appropriate here
  * I need to do a bit more reading about "variable importance metrics" and how reliable they are. 
  * I need to do a bit more reading about how elastic net works to make sure I know what its notion of "variable importance" means more rigorously, and how I can coerce it to give me p-values (which journals care about). 
  
# Session Info
  
```{r}
sessionInfo()
```
  
