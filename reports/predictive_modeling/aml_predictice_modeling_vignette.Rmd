---
title: "AML Predictive Modeling Vignette" 
author: tkeyes
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
# Libraries
libraries <- 
  c(
    "tidyverse", 
    "tidymodels", 
    "rlang", 
    "ggthemes"
  )

source(here::here("scripts", "setup", "aml_utils.R")) #may need to change on scg machines
call_libraries(libraries)

# Parameters
set_global_variables(locale = "galaxia")
md_path <- here::here("data-raw", "AML_metadata.xlsx")
tidyTOF_directory <- file.path("~", "GitHub", "tidyTOF")

CLUSTER_OUTPUT <- here::here("data", "cancer_data_clustered.rds")

# Sourcing tidyTOF functions
source_tidyTOF(tidyTOF_directory)

# set up aml marker and patient information
marker_setup()
patient_setup()

# Misc globals for running different parts of this vignette
mah_dev_path <- here::here("data", "mahalanobis_dev_features_10.rds")
cos_dev_path <- here::here("data", "cosine_dev_features_10.rds")
mah_citrus_path <- here::here("data", "mahalanobis_citrus_features.rds")
cos_citrus_path <- here::here("data", "cosine_citrus_features.rds")
md_path <- here::here("data", "md.rds")

DDPR_PROP <- 0.2

```

# Read in data

```{r}
metadata <- 
  md_path %>% 
  read_rds() %>% 
  transmute(
    patient, 
    age = age_at_diagnosis_in_days, 
    wbc = wbc_at_diagnosis, 
    outcome = first_event %>% str_replace("Censored", "Non-relapse")
  )

mah_dev_data <- 
  mah_dev_path %>% 
  read_rds() %>% 
  filter(condition == "dx") %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)

cos_dev_data <- 
  cos_dev_path %>% 
  read_rds() %>% 
  filter(condition == "dx") %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)

mah_citrus_data <- 
  mah_citrus_path %>% 
  read_rds() %>% 
  filter(condition == "dx") %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)

cos_citrus_data <- 
  cos_citrus_path %>% 
  read_rds() %>% 
  filter(condition == "dx") %>% 
  left_join(metadata, by = "patient") %>% 
  replace(list = is.na(.), values = 0)
```


# Very quick EDA


## Mahalanobis developmental classifier features 

```{r}
# needs to show the percentage variance for each PC in the plot

pca_recipe <- 
  recipe(
    ~., 
    data = 
      mah_dev_data %>% 
      replace(list = is.na(.), values = 0)
  ) %>% 
  step_zv(all_numeric()) %>% 
  step_normalize(all_numeric()) %>% 
  step_pca(all_numeric(), num_comp = 5) 

pca_prep <- prep(pca_recipe)

pca_result <- juice(pca_prep)

pca_result %>% 
  ggplot(aes(x = PC1, y = PC2, color = outcome)) + 
  geom_point(size = 3, alpha = 0.7) + 
  labs(title = "Developmental Classifier Features using Mahalanobis Distance")
```


```{r, warning = FALSE}
mah_dev_data %>% 
  pivot_longer(
    cols = c(-patient, -condition, -outcome), 
    names_to = "feature", 
    values_to = "value"
  ) %>% 
  group_by(feature) %>% 
  nest() %>%
  mutate(
    plots = 
      map2(
        .x = data, 
        .y = feature,
        .f = 
          ~ ggplot(
            aes(x = fct_reorder(patient, value), y = value, color = outcome), 
            data = .x
          ) + 
          geom_point() + 
          coord_flip() + 
          labs(title = feature)
      )
  ) %>% 
  pull(plots) %>% 
  walk(.f = print)


```


## Cosine developmental classifier features

```{r}
# needs to show the percentage variance for each PC in the plot

pca_recipe <- 
  recipe(
    ~., 
    data = 
      cos_dev_data %>% 
      replace(list = is.na(.), values = 0)
  ) %>% 
  step_zv(all_numeric()) %>% 
  step_normalize(all_numeric()) %>% 
  step_pca(all_numeric(), num_comp = 5) 

pca_prep <- prep(pca_recipe)

pca_result <- juice(pca_prep)

pca_result %>% 
  ggplot(aes(x = PC1, y = PC2, color = outcome)) + 
  geom_point(size = 3, alpha = 0.7)  + 
  labs(title = "Developmental Classifier Features using Cosine Distance")
```

# Split data 

```{r}
mah_data_split <- initial_split(mah_dev_data, prop = 0.8, strata = outcome)
mah_data_train <- training(mah_data_split)
mah_data_test <- testing(mah_data_split)

cos_data_split <- initial_split(cos_dev_data, prop = 0.8, strata = outcome)
cos_data_train <- training(cos_data_split)
cos_data_test <- testing(cos_data_split)
```


# Replication of DDPR Elastic Net Model

```{r}
ddpr_logistic <-
  logistic_reg(mode = "classification", mixture = 0.5) %>% 
  set_engine("glmnet")

mah_ddpr_fit <- 
  fit_xy(
    ddpr_logistic, 
    x = 
      select(mah_data_train, -patient, -condition, -outcome),
    y = 
      pull(mah_data_train, outcome) %>% 
      as.factor()
  )

my_predictions <- 
  mah_ddpr_fit %>% 
  multi_predict(select(mah_data_test, -patient, -condition, -outcome))

prediction_table <- 
  my_predictions %>% 
  mutate(
    patient = mah_data_test$patient, 
    .pred = 
      map2(
        .x = patient, 
        .y = .pred, 
        .f = 
          ~ mutate(.y, patient = .x)
      )
  ) %>% 
  select(-patient) %>% 
  unnest(c(.pred)) %>% 
  rename(predicted_outcome = .pred_class) %>% 
  left_join(mah_data_test %>% select(patient, outcome)) %>% 
  group_by(penalty) %>% 
  summarize(percent_correct = mean(predicted_outcome == outcome) * 100) %>% 
  arrange(desc(percent_correct))

prediction_table
```

```{r}
class(prediction_table)
```

For the highest accuracy, choose the most highly penalized model and go with that one. 

```{r}
best_penalty <- 
  prediction_table %>% 
  slice_max(order_by = percent_correct) %>% 
  slice_max(order_by = penalty) %>% 
  pull(penalty)
```

```{r}
mah_ddpr_fit %>% 
  pluck("fit") %>% 
  coef(s = best_penalty) %>% 
  as.matrix() %>% 
  as_tibble(rownames = "marker_name") %>% 
  rename(coefficient = `1`) %>% 
  filter(coefficient > 0.000000000000)

```


# CITRUS features Elastic Net Model

...to come
